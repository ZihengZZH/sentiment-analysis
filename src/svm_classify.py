import numpy as np
import os
import math
import datetime
import subprocess
import progressbar
import statsmodels.api
from scipy import stats
from sklearn import svm
from sklearn import metrics
from sklearn.model_selection import GridSearchCV
from smart_open import smart_open
from collections import namedtuple

import src.text as text
import src.bow_feat as feat
import src.doc2vec as doc2vec
import src.cv_partition as cv
import src.stats_test as st

'''
The SVM classifier is achieved by using SVM-Light implmenetation by Joachims, 1999
subporocess.run(args) run the command described by args. Wait for command to complete, then return a CompletedProcess instance

HOW TO USE SVM-LIGHT
svm_learn example1/train.dat example1/model
svm_classify example1/test.dat example1/model example1/predictions
'''

SVM_BOW_PATH = './models/svm_models/bow_svm/'
SVM_DOC2VEC_PATH = './models/svm_models/doc2vec_svm/'
SVM_PARA_GRID_PATH = './results/gridsearch_doc2vec.txt'
SVM_PARA_BOW_GRID_PATH = './results/gridsearch_bow.txt'
parameters = namedtuple("Parameters", "kernel C gamma")
svm_para_dbow = parameters(2, 100, 0.001)
svm_para_dm = parameters(2, 10, 0.0001)
svm_para_unigram = parameters(0, 1, 0)
svm_para_bigram = parameters(2, 10, 0.001)


# prepare the train/text data for SVM classifier
def prepare_data(train_test_matrix, train_test_size, if_doc2vec, test=False):
    # para train_test_matrix: training/test matrix generated by bow_feat / doc2vec
    # para train_test_size: training/test set size
    # para if_doc2vec: bow features or doc2vec embeddings
    # para test: either training data or test data preparation
    # type train_test_matrix: list(list(int/float))
    if not if_doc2vec:
        path = SVM_BOW_PATH + 'test.dat' if test else SVM_BOW_PATH + 'train.dat'
    else:
        path = SVM_DOC2VEC_PATH + 'test.dat' if test else SVM_DOC2VEC_PATH + 'train.dat'
    
    if os.path.isfile(path):
        os.remove(path) # delete the old file

    # training or test vector of labels 
    train_test_labels = np.hstack((np.ones(train_test_size)*-1, np.ones(train_test_size)))
    train_test_data = list()
    feat_size = len(train_test_matrix[0])

    bar = progressbar.ProgressBar()
    with smart_open(path, 'w', encoding='utf-8') as f:
        for i in bar(range(len(train_test_labels))):
            train_test_data.append(["%d:%f" % (j+1, train_test_matrix[i][j]) for j in range(feat_size)])
            # write the label and data to file
            f.write("%d " % train_test_labels[i])
            for k in range(len(train_test_data[i])):
                f.write("%s " % train_test_data[i][k])
            f.write("\n")


# save the readme file of SVM model
def save_readme(if_doc2vec=False):
    # para if_doc2vec: bow features or doc2vec embeddings
    readme_notes = np.array(["This SVM model is trained on ", str(datetime.datetime.now())])
    if if_doc2vec:
        np.savetxt(SVM_DOC2VEC_PATH + "readme.txt", readme_notes, fmt="%s")
    else:
        np.savetxt(SVM_BOW_PATH + "readme.txt", readme_notes, fmt="%s")


# train the SVM-Light classifier 
def train_svm_classifier(if_doc2vec, dbow, feature_type):
    # para if_doc2vec: bow features or doc2vec embeddings
    svm_path = SVM_DOC2VEC_PATH if if_doc2vec else SVM_BOW_PATH
    # check if files available
    assert os.path.isfile("./svm-light/svm_learn"), "SVM Light source code missing"
    assert os.path.isfile(svm_path + "train.dat"), "SVM training data missing"
    if if_doc2vec:
        (kernel, C, gamma) = (svm_para_dbow.kernel, svm_para_dbow.C, svm_para_dbow.gamma) if dbow else (svm_para_dm.kernel, svm_para_dm.C, svm_para_dm.gamma)
    else:
        (kernel, C, gamma) = (svm_para_unigram.kernel, svm_para_unigram.C, svm_para_unigram.gamma) if feature_type == 'unigram' else (svm_para_bigram.kernel, svm_para_bigram.C, svm_para_bigram.gamma)
    # run the SVM-Light classifier
    if kernel == 0:
        subprocess.run(["./svm-light/svm_learn", "-t", "%d" % kernel, "-c", "%f" % C, "-m", "100", svm_path + "train.dat", svm_path + "model"])
    elif kernel == 2:
        subprocess.run(["./svm-light/svm_learn", "-t", "%d" % kernel, "-c", "%f" % C, "-g", "%f" % gamma, "-m", "100", svm_path + "train.dat", svm_path + "model"])
    save_readme(if_doc2vec)
    

# test the SVM-Light classifier 
def test_svm_classifier(if_doc2vec):
    # para if_doc2vec: bow features or doc2vec embeddings
    svm_path = SVM_DOC2VEC_PATH if if_doc2vec else SVM_BOW_PATH
    # check if files available
    assert os.path.isfile("./svm-light/svm_classify"), "SVM Light source code missing"
    assert os.path.isfile(svm_path + "test.dat"), "SVM test data missing"
    assert os.path.isfile(svm_path + "model"), "SVM model data missing (SVM classifier is perhaps not trained yet.)"
    # run the SVM-Light classifier
    subprocess.run(["./svm-light/svm_classify", svm_path + "test.dat", svm_path + "model", svm_path + "predictions"])


def SVM_classifier(feature_type, cv_part=False, if_doc2vec=False, model_no=1, concatenate=False, train_size_cv=None, test_size_cv=None, reviews_train_cv=None, reviews_test_cv=None):
    # para feature_type: unigram or bigram (n-gram bag of words)
    # para cv_part: whether or not is partitioned with cross-validation
    # para if_doc2vec: bow features or doc2vec embeddings
    # para model_no: which doc2vec model to use / list if concatenate
    # para concatenate: whether to concatenate two doc2vec models
    print("\nSVM-Light Classifier on sentiment detection running\n\npreparing data ...")
    if not cv_part:
        train_size, test_size, reviews_train, reviews_test = cv.prepare_data()
    else:
        train_size, test_size, reviews_train, reviews_test = train_size_cv, test_size_cv, reviews_train_cv, reviews_test_cv

    if not if_doc2vec:
        print("\nfinding the corpus for the classifier ...")
        # full vocabulary for the training reviews (frequency cutoff implemented)
        if feature_type == 'unigram':
            full_vocab = feat.get_vocab(reviews_train, cutoff_threshold=8)
        elif feature_type == 'bigram':
            full_vocab = feat.get_vocab_bigram(reviews_train, cutoff_threshold=13)
        else:
            full_vocab = feat.get_vocab(reviews_train, cutoff_threshold=8) + feat.get_vocab_bigram(reviews_train, cutoff_threshold=13)
        vocab_length = len(full_vocab)
        print("\n#features is ", vocab_length)

        print("\ngenerate the BOW-based training matrix ...")
        # training matrix of data
        if feature_type == 'unigram':
            train_matrix = feat.bag_words2vec_unigram(full_vocab, reviews_train)
        elif feature_type == 'bigram':
            train_matrix = feat.bag_words2vec_bigram(full_vocab, reviews_train)
        else:
            train_matrix = feat.concatenate_feat(feat.bag_words2vec_unigram(
                full_vocab, reviews_train), feat.bag_words2vec_bigram(full_vocab, reviews_train))
        print('\ndescription of training matrix', stats.describe(train_matrix))

        print("\ngenerate the BOW-based test matrix ...")
        # testing matrix of data
        if feature_type == 'unigram':
            test_matrix = feat.bag_words2vec_unigram(full_vocab, reviews_test)
        elif feature_type == 'bigram':
            test_matrix = feat.bag_words2vec_bigram(full_vocab, reviews_test)
        else:
            test_matrix = feat.concatenate_feat(feat.bag_words2vec_unigram(
                full_vocab, reviews_test), feat.bag_words2vec_bigram(full_vocab, reviews_test))
        print('\ndescription of test matrix', stats.describe(test_matrix))
    
    else:
        if concatenate:
            print("\ninfer document embeddings with pre-trained no.%d doc2vec model and no.%d doc2vec model ..." % (model_no[0], model_no[1]))
            train_matrix, train_labels = doc2vec.infer_embedding(model_no, reviews_train, train_size, concatenate)
            test_matrix, test_labels = doc2vec.infer_embedding(model_no, reviews_test, test_size, concatenate)
        else:
            print("\ninfer document embeddings with pre-trained no.%d doc2vec model ..." % model_no)
            train_matrix, train_labels = doc2vec.infer_embedding(model_no, reviews_train, train_size, False)
            test_matrix, test_labels = doc2vec.infer_embedding(model_no, reviews_test, test_size, False)

    print("\nprepare the data for the SVM-Light classifier ...")
    prepare_data(train_matrix, train_size, if_doc2vec)
    prepare_data(test_matrix, test_size, if_doc2vec, test=True)
    print("\ndata preparation, DONE")

    # check the algorithm of doc2vec model
    if model_no == 18 or model_no == 20:
        dbow = True
    elif model_no == 15:
        dbow = False
    elif concatenate:
        dbow = False
    else:
        dbow = None
    
    print("\ntrain the SVM-Light classifier ... \n")
    train_svm_classifier(if_doc2vec, dbow, feature_type)
    print("\ntraining, DONE.")
    
    print("\ntest the SVM-Light classifier ... \n")
    test_svm_classifier(if_doc2vec)
    print("\ntest, DONE.")

    print("\nclassification results are shown as above")

    def read_predictions():
        svm_path = SVM_DOC2VEC_PATH if if_doc2vec else SVM_BOW_PATH
        misclassify, results = 0, []
        with smart_open(svm_path + 'predictions', 'r') as f:
            for line in f:
                line = line.strip()
                results.append(float(line))
        for i in range(len(results)):
            if i < test_size and results[i] > 0:
                misclassify += 1
            elif i > test_size and results[i] < 0:
                misclassify += 1
        # return (len(results) - misclassify) / len(results)
        return results
    
    return read_predictions()


def SVM_classifier_sklearn(model_no, train_size, test_size, reviews_train, reviews_test):
    # para model_no: which Doc2Vec model to use
    if_doc2vec = False
    if if_doc2vec:
        train_size, test_size, reviews_train, reviews_test = cv.prepare_data()
        train_matrix, train_labels = doc2vec.infer_embedding(model_no, reviews_train, train_size, False)
        test_matrix, test_labels = doc2vec.infer_embedding(model_no, reviews_test, test_size, False)
    else:
        full_vocab = feat.get_vocab(reviews_train, cutoff_threshold=8)
        print("\nfeature size: %d" % len(full_vocab))
        train_matrix = feat.bag_words2vec_unigram(full_vocab, reviews_train)
        test_matrix = feat.bag_words2vec_unigram(full_vocab, reviews_test)
        train_labels = np.hstack((np.ones(train_size)*-1, np.ones(train_size)))
        test_labels = np.hstack((np.ones(test_size)*-1, np.ones(test_size)))
    
    X_train, y_train, X_test, y_test = train_matrix, train_labels, test_matrix, test_labels
    print("\ntrain the sklearn SVM classifier ... \n")
    svm_doc2vec = svm.SVC(kernel='linear', C=10)
    svm_doc2vec.fit(X_train, y_train)
    print("\ntraining, DONE.")
    print("\ntest the sklearn SVM classifier ... \n")
    y_pred = svm_doc2vec.predict(X_test)
    print(metrics.classification_report(y_test, y_pred))
    print("Overall accuracy: ", round(metrics.accuracy_score(y_test, y_pred), 2))
    return round(metrics.accuracy_score(y_test, y_pred), 2), y_pred


def SVM_grid_search_doc2vec(model_no, concatenate):
    # para model_no: which Doc2Vec model to use
    train_size, test_size, reviews_train, reviews_test = cv.prepare_data()
    train_matrix, train_labels = doc2vec.infer_embedding(model_no, reviews_train, train_size, concatenate)
    test_matrix, test_labels = doc2vec.infer_embedding(model_no, reviews_test, test_size, concatenate)
    X_train, y_train, X_test, y_test = train_matrix, train_labels, test_matrix, test_labels

    print("\nrun the Grid Search for SVM classifier (with doc2vec embeddings) ... \n")
    tuned_parameters = [
        {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]},
        {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}
        ]
    clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=3, n_jobs=12, verbose=10)
    clf.fit(X_train, y_train)
    print("\ngrid search, DONE.")
    print("\ngrid search results are listed as follows.")
    print(clf.best_params_, clf.best_score_)
    sorted(clf.cv_results_.keys())
    
    with smart_open(SVM_PARA_GRID_PATH, 'a+', encoding='utf-8') as f:
        model_name = str(doc2vec.load_model(model_no)) if not concatenate else (str(doc2vec.load_model(model_no[0])) + str(doc2vec.load_model(model_no[1])))
        line = model_name + str(clf.best_params_) + "\t" + str(clf.best_score_)
        f.write(line)
        f.write("\n")
    print("\ngrid search results are written to file.")


def SVM_grid_search_bow(feature_type):
    # para feature_type: either unigram or bigram to use
    train_size, test_size, reviews_train, reviews_test = cv.prepare_data()
    if feature_type == 'unigram':
        full_vocab = feat.get_vocab(reviews_train, cutoff_threshold=8)
    elif feature_type == 'bigram':
        full_vocab = feat.get_vocab_bigram(reviews_train, cutoff_threshold=13)
    else:
        full_vocab = feat.get_vocab(reviews_train, cutoff_threshold=8) + feat.get_vocab_bigram(reviews_train, cutoff_threshold=13)
    vocab_length = len(full_vocab)
    print("\n#features is ", vocab_length)

    print("\ngenerate the BOW-based training matrix ...")
    # training matrix of data
    if feature_type == 'unigram':
        train_matrix = feat.bag_words2vec_unigram(full_vocab, reviews_train)
    elif feature_type == 'bigram':
        train_matrix = feat.bag_words2vec_bigram(full_vocab, reviews_train)
    else:
        train_matrix = feat.concatenate_feat(feat.bag_words2vec_unigram(
            full_vocab, reviews_train), feat.bag_words2vec_bigram(full_vocab, reviews_train))
    print('\ndescription of training matrix', stats.describe(train_matrix))

    print("\ngenerate the BOW-based test matrix ...")
    # testing matrix of data
    if feature_type == 'unigram':
        test_matrix = feat.bag_words2vec_unigram(full_vocab, reviews_test)
    elif feature_type == 'bigram':
        test_matrix = feat.bag_words2vec_bigram(full_vocab, reviews_test)
    else:
        test_matrix = feat.concatenate_feat(feat.bag_words2vec_unigram(
            full_vocab, reviews_test), feat.bag_words2vec_bigram(full_vocab, reviews_test))
    print('\ndescription of test matrix', stats.describe(test_matrix))

    train_labels = np.hstack((np.ones(train_size)*-1, np.ones(train_size)))
    X_train, y_train = train_matrix, train_labels

    print("\nrun the Grid Search for SVM classifier (with bow features) ... \n")
    tuned_parameters = [
        {'kernel': ['rbf'], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4], 'C': [10, 100, 1000, 10000]},
        {'kernel': ['linear'], 'C': [10, 100, 1000, 10000]}
        ]
    clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=5, n_jobs=12, verbose=10)
    clf.fit(X_train, y_train)
    print("\ngrid search, DONE.")
    print("\ngrid search results are listed as follows.")
    print(clf.best_params_, clf.best_score_)
    sorted(clf.cv_results_.keys())
    
    with smart_open(SVM_PARA_BOW_GRID_PATH, 'a+', encoding='utf-8') as f:
        line = str(feature_type) + "\t" + str(clf.best_params_) + "\t" + str(clf.best_score_)
        f.write(line)
        f.write("\n")
    print("\ngrid search results are written to file.")


def logistic_predictor(model_no):
    train_size, test_size, reviews_train, reviews_test = cv.prepare_data()
    train_matrix, train_labels = doc2vec.infer_embedding(model_no, reviews_train, train_size, False)
    test_matrix, test_labels = doc2vec.infer_embedding(model_no, reviews_test, test_size, False)
    # X_train, y_train, X_test, y_test = train_matrix, train_labels, test_matrix, test_labels

    logit = statsmodels.api.Logit(train_labels, train_matrix)
    predictor = logit.fit(disp=0)
    train_prediction = predictor.predict(train_matrix)
    cor = sum(np.rint(train_prediction) == train_labels)
    print("training accuracy", cor/len(train_prediction))
    test_predictions = predictor.predict(test_matrix)
    corrects = sum(np.rint(test_predictions) == test_labels)
    errors = len(test_predictions) - corrects
    error_rate = float(errors) / len(test_predictions)
    print("accuracy", corrects/len(test_predictions))


def save_results(feat_type, doc2vec, train_size, classification):
    if_doc2vec = 1 if doc2vec else 0
    notes = "results obtained on " + str(datetime.datetime.now())
    f = open('./results/results.txt', 'a+', encoding='utf-8')
    f.write("\nfeature: %s\tif_doc2vec: %d\ttraining size: %d\tclassification_accuracy: %f\tnotes: %s" % (
        feat_type, if_doc2vec, train_size, classification, notes))
    f.close()


def save_results_cv(fold_type, feat_type, doc2vec, results, performances, perf_average, variance):
    if_doc2vec = 1 if doc2vec else 0
    notes = "results obtained on " + str(datetime.datetime.now())
    f = open('./results/results_cv.txt', 'a+', encoding='utf-8')
    f.write("fold type: %s\nfeature: %s\t#performance: %s\taverage performance: %f\tvariance: %f\tnotes: %s\tdoc2vec: %d\n" % (fold_type, feat_type, performances, perf_average, variance, notes, if_doc2vec))
    f.write(str(results))
    f.write('\n')
    f.close()


'''
./svm-light/svm_learn [option] example_file model_file

-t (int)
0: linear (default)
1: polynomial (s a*b+c)^d
2. radial basis function exp(-gamma ||a-b||^2)
3. sigmoid tanh(s a*b + c)
4. user defined kernel from kernel.h

-g (float)
parameter gamma in rbf kernel

'''