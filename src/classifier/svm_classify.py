import os
import math
import json
import datetime
import subprocess
import progressbar
import statsmodels.api
import numpy as np
from scipy import stats
from sklearn import svm
from sklearn import metrics
from sklearn.model_selection import GridSearchCV
from smart_open import smart_open
from collections import namedtuple

import src.text as text
import src.bow_feat as feat
import src.doc2vec as doc2vec
import src.cv_partition as cv
import src.stats_test as st

'''
The SVM classifier is achieved by using SVM-Light implmenetation by Joachims, 1999
subporocess.run(args) run the command described by args. Wait for command to complete, then return a CompletedProcess instance

HOW TO USE SVM-LIGHT
svm_learn example1/train.dat example1/model
svm_classify example1/test.dat example1/model example1/predictions
'''

config = json.load(open('./config.json', 'r'))
parameters = namedtuple("Parameters", "kernel C gamma")

temp_1 = config['svm_model']['svm_para_dbow']
svm_para_dbow = parameters(temp_1[0], temp_1[1], temp_1[2])
temp_2 = config['svm_model']['svm_para_dm']
svm_para_dm = parameters(temp_2[0], temp_2[1], temp_2[2])
temp_3 = config['svm_model']['svm_para_unigram']
svm_para_unigram = parameters(temp_3[0], temp_3[1],temp_3[2])
temp_4 = config['svm_model']['svm_para_bigram']
svm_para_bigram = parameters(temp_4[0], temp_4[1],temp_4[2])

SVM_BOW_PATH = config['svm_model']['svm_bow_path']
SVM_DOC2VEC_PATH = config['svm_model']['svm_doc2vec_path']
SVM_PARA_GRID_PATH = config['svm_model']['svm_para_grid_doc2vec_path']
SVM_PARA_BOW_GRID_PATH = config['svm_model']['svm_para_grid_bow_path']


# prepare the train/text data for SVM classifier
def prepare_data(train_test_matrix, train_test_size, if_doc2vec, test=False):
    # para train_test_matrix: training/test matrix generated by bow_feat / doc2vec
    # para train_test_size: training/test set size
    # para if_doc2vec: bow features or doc2vec embeddings
    # para test: either training data or test data preparation
    # type train_test_matrix: list(list(int/float))
    if not if_doc2vec:
        path = SVM_BOW_PATH + 'test.dat' if test else SVM_BOW_PATH + 'train.dat'
    else:
        path = SVM_DOC2VEC_PATH + 'test.dat' if test else SVM_DOC2VEC_PATH + 'train.dat'
    
    if os.path.isfile(path):
        os.remove(path) # delete the old file

    # training or test vector of labels 
    train_test_labels = np.hstack((np.ones(train_test_size)*-1, np.ones(train_test_size)))
    train_test_data = list()
    feat_size = len(train_test_matrix[0])

    bar = progressbar.ProgressBar()
    with smart_open(path, 'w', encoding='utf-8') as f:
        for i in bar(range(len(train_test_labels))):
            train_test_data.append(["%d:%f" % (j+1, train_test_matrix[i][j]) for j in range(feat_size)])
            # write the label and data to file
            f.write("%d " % train_test_labels[i])
            for k in range(len(train_test_data[i])):
                f.write("%s " % train_test_data[i][k])
            f.write("\n")


# save the readme file of SVM model
def save_readme(if_doc2vec=False):
    # para if_doc2vec: bow features or doc2vec embeddings
    readme_notes = np.array(["This SVM model is trained on ", str(datetime.datetime.now())])
    if if_doc2vec:
        np.savetxt(SVM_DOC2VEC_PATH + "readme.txt", readme_notes, fmt="%s")
    else:
        np.savetxt(SVM_BOW_PATH + "readme.txt", readme_notes, fmt="%s")


# train the SVM-Light classifier 
def train_svm_classifier(if_doc2vec, dbow, feature_type):
    # para if_doc2vec: bow features or doc2vec embeddings
    svm_path = SVM_DOC2VEC_PATH if if_doc2vec else SVM_BOW_PATH
    # check if files available
    assert os.path.isfile("./svm-light/svm_learn"), "SVM Light source code missing"
    assert os.path.isfile(svm_path + "train.dat"), "SVM training data missing"
    if if_doc2vec:
        (kernel, C, gamma) = (svm_para_dbow.kernel, svm_para_dbow.C, svm_para_dbow.gamma) if dbow else (svm_para_dm.kernel, svm_para_dm.C, svm_para_dm.gamma)
    else:
        (kernel, C, gamma) = (svm_para_unigram.kernel, svm_para_unigram.C, svm_para_unigram.gamma) if feature_type == 'unigram' else (svm_para_bigram.kernel, svm_para_bigram.C, svm_para_bigram.gamma)
    # run the SVM-Light classifier
    if kernel == 0:
        subprocess.run(["./svm-light/svm_learn", "-t", "%d" % kernel, "-c", "%f" % C, "-m", "100", svm_path + "train.dat", svm_path + "model"])
    elif kernel == 2:
        subprocess.run(["./svm-light/svm_learn", "-t", "%d" % kernel, "-c", "%f" % C, "-g", "%f" % gamma, "-m", "100", svm_path + "train.dat", svm_path + "model"])
    save_readme(if_doc2vec)
    

# test the SVM-Light classifier 
def test_svm_classifier(if_doc2vec):
    # para if_doc2vec: bow features or doc2vec embeddings
    svm_path = SVM_DOC2VEC_PATH if if_doc2vec else SVM_BOW_PATH
    # check if files available
    assert os.path.isfile("./svm-light/svm_classify"), "SVM Light source code missing"
    assert os.path.isfile(svm_path + "test.dat"), "SVM test data missing"
    assert os.path.isfile(svm_path + "model"), "SVM model data missing (SVM classifier is perhaps not trained yet.)"
    # run the SVM-Light classifier
    subprocess.run(["./svm-light/svm_classify", svm_path + "test.dat", svm_path + "model", svm_path + "predictions"])


def SVM_classifier(feature_type, cv_part=False, if_doc2vec=False, model_no=1, concatenate=False, train_size_cv=None, test_size_cv=None, reviews_train_cv=None, reviews_test_cv=None):
    # para feature_type: unigram or bigram (n-gram bag of words)
    # para cv_part: whether or not is partitioned with cross-validation
    # para if_doc2vec: bow features or doc2vec embeddings
    # para model_no: which doc2vec model to use / list if concatenate
    # para concatenate: whether to concatenate two doc2vec models
    print("\nSVM-Light Classifier on sentiment detection running\n\npreparing data ...")
    if not cv_part:
        train_size, test_size, reviews_train, reviews_test = cv.prepare_data()
    else:
        train_size, test_size, reviews_train, reviews_test = train_size_cv, test_size_cv, reviews_train_cv, reviews_test_cv

    if not if_doc2vec:
        print("\nfinding the corpus for the classifier ...")
        # full vocabulary for the training reviews (frequency cutoff implemented)
        if feature_type == 'unigram':
            full_vocab = feat.get_vocab(reviews_train, cutoff_threshold=8)
        elif feature_type == 'bigram':
            full_vocab = feat.get_vocab_bigram(reviews_train, cutoff_threshold=13)
        else:
            full_vocab = feat.get_vocab(reviews_train, cutoff_threshold=8) + feat.get_vocab_bigram(reviews_train, cutoff_threshold=13)
        vocab_length = len(full_vocab)
        print("\n#features is ", vocab_length)

        print("\ngenerate the BOW-based training matrix ...")
        # training matrix of data
        if feature_type == 'unigram':
            train_matrix = feat.bag_words2vec_unigram(full_vocab, reviews_train)
        elif feature_type == 'bigram':
            train_matrix = feat.bag_words2vec_bigram(full_vocab, reviews_train)
        else:
            train_matrix = feat.concatenate_feat(feat.bag_words2vec_unigram(
                full_vocab, reviews_train), feat.bag_words2vec_bigram(full_vocab, reviews_train))
        print('\ndescription of training matrix', stats.describe(train_matrix))

        print("\ngenerate the BOW-based test matrix ...")
        # testing matrix of data
        if feature_type == 'unigram':
            test_matrix = feat.bag_words2vec_unigram(full_vocab, reviews_test)
        elif feature_type == 'bigram':
            test_matrix = feat.bag_words2vec_bigram(full_vocab, reviews_test)
        else:
            test_matrix = feat.concatenate_feat(feat.bag_words2vec_unigram(
                full_vocab, reviews_test), feat.bag_words2vec_bigram(full_vocab, reviews_test))
        print('\ndescription of test matrix', stats.describe(test_matrix))
    
    else:
        if concatenate:
            print("\ninfer document embeddings with pre-trained no.%d doc2vec model and no.%d doc2vec model ..." % (model_no[0], model_no[1]))
            train_matrix, train_labels = doc2vec.infer_embedding(model_no, reviews_train, train_size, concatenate)
            test_matrix, test_labels = doc2vec.infer_embedding(model_no, reviews_test, test_size, concatenate)
        else:
            print("\ninfer document embeddings with pre-trained no.%d doc2vec model ..." % model_no)
            train_matrix, train_labels = doc2vec.infer_embedding(model_no, reviews_train, train_size, False)
            test_matrix, test_labels = doc2vec.infer_embedding(model_no, reviews_test, test_size, False)

    print("\nprepare the data for the SVM-Light classifier ...")
    prepare_data(train_matrix, train_size, if_doc2vec)
    prepare_data(test_matrix, test_size, if_doc2vec, test=True)
    print("\ndata preparation, DONE")

    # check the algorithm of doc2vec model
    if model_no == 18 or model_no == 20:
        dbow = True
    elif model_no == 15:
        dbow = False
    elif concatenate:
        dbow = False
    else:
        dbow = None
    
    print("\ntrain the SVM-Light classifier ... \n")
    train_svm_classifier(if_doc2vec, dbow, feature_type)
    print("\ntraining, DONE.")
    
    print("\ntest the SVM-Light classifier ... \n")
    test_svm_classifier(if_doc2vec)
    print("\ntest, DONE.")

    print("\nclassification results are shown as above")

    def read_predictions():
        svm_path = SVM_DOC2VEC_PATH if if_doc2vec else SVM_BOW_PATH
        misclassify, results = 0, []
        with smart_open(svm_path + 'predictions', 'r') as f:
            for line in f:
                line = line.strip()
                results.append(float(line))
        for i in range(len(results)):
            if i < test_size and results[i] > 0:
                misclassify += 1
            elif i > test_size and results[i] < 0:
                misclassify += 1
        # return (len(results) - misclassify) / len(results)
        return results
    
    return read_predictions()


def SVM_classifier_sklearn(model_no, train_size, test_size, reviews_train, reviews_test):
    # para model_no: which Doc2Vec model to use
    if_doc2vec = False
    if if_doc2vec:
        train_size, test_size, reviews_train, reviews_test = cv.prepare_data()
        train_matrix, train_labels = doc2vec.infer_embedding(model_no, reviews_train, train_size, False)
        test_matrix, test_labels = doc2vec.infer_embedding(model_no, reviews_test, test_size, False)
    else:
        full_vocab = feat.get_vocab(reviews_train, cutoff_threshold=8)
        print("\nfeature size: %d" % len(full_vocab))
        train_matrix = feat.bag_words2vec_unigram(full_vocab, reviews_train)
        test_matrix = feat.bag_words2vec_unigram(full_vocab, reviews_test)
        train_labels = np.hstack((np.ones(train_size)*-1, np.ones(train_size)))
        test_labels = np.hstack((np.ones(test_size)*-1, np.ones(test_size)))
    
    X_train, y_train, X_test, y_test = train_matrix, train_labels, test_matrix, test_labels
    print("\ntrain the sklearn SVM classifier ... \n")
    svm_doc2vec = svm.SVC(kernel='linear', C=10)
    svm_doc2vec.fit(X_train, y_train)
    print("\ntraining, DONE.")
    print("\ntest the sklearn SVM classifier ... \n")
    y_pred = svm_doc2vec.predict(X_test)
    print(metrics.classification_report(y_test, y_pred))
    print("Overall accuracy: ", round(metrics.accuracy_score(y_test, y_pred), 2))
    return round(metrics.accuracy_score(y_test, y_pred), 2), y_pred


def SVM_grid_search_doc2vec(model_no, concatenate):
    # para model_no: which Doc2Vec model to use
    train_size, test_size, reviews_train, reviews_test = cv.prepare_data()
    train_matrix, train_labels = doc2vec.infer_embedding(model_no, reviews_train, train_size, concatenate)
    test_matrix, test_labels = doc2vec.infer_embedding(model_no, reviews_test, test_size, concatenate)
    X_train, y_train, X_test, y_test = train_matrix, train_labels, test_matrix, test_labels

    print("\nrun the Grid Search for SVM classifier (with doc2vec embeddings) ... \n")
    tuned_parameters = [
        {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]},
        {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}
        ]
    clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=3, n_jobs=12, verbose=10)
    clf.fit(X_train, y_train)
    print("\ngrid search, DONE.")
    print("\ngrid search results are listed as follows.")
    print(clf.best_params_, clf.best_score_)
    sorted(clf.cv_results_.keys())
    
    with smart_open(SVM_PARA_GRID_PATH, 'a+', encoding='utf-8') as f:
        model_name = str(doc2vec.load_model(model_no)) if not concatenate else (str(doc2vec.load_model(model_no[0])) + str(doc2vec.load_model(model_no[1])))
        line = model_name + str(clf.best_params_) + "\t" + str(clf.best_score_)
        f.write(line)
        f.write("\n")
    print("\ngrid search results are written to file.")


def SVM_grid_search_bow(feature_type):
    # para feature_type: either unigram or bigram to use
    train_size, test_size, reviews_train, reviews_test = cv.prepare_data()
    if feature_type == 'unigram':
        full_vocab = feat.get_vocab(reviews_train, cutoff_threshold=8)
    elif feature_type == 'bigram':
        full_vocab = feat.get_vocab_bigram(reviews_train, cutoff_threshold=13)
    else:
        full_vocab = feat.get_vocab(reviews_train, cutoff_threshold=8) + feat.get_vocab_bigram(reviews_train, cutoff_threshold=13)
    vocab_length = len(full_vocab)
    print("\n#features is ", vocab_length)

    print("\ngenerate the BOW-based training matrix ...")
    # training matrix of data
    if feature_type == 'unigram':
        train_matrix = feat.bag_words2vec_unigram(full_vocab, reviews_train)
    elif feature_type == 'bigram':
        train_matrix = feat.bag_words2vec_bigram(full_vocab, reviews_train)
    else:
        train_matrix = feat.concatenate_feat(feat.bag_words2vec_unigram(
            full_vocab, reviews_train), feat.bag_words2vec_bigram(full_vocab, reviews_train))
    print('\ndescription of training matrix', stats.describe(train_matrix))

    print("\ngenerate the BOW-based test matrix ...")
    # testing matrix of data
    if feature_type == 'unigram':
        test_matrix = feat.bag_words2vec_unigram(full_vocab, reviews_test)
    elif feature_type == 'bigram':
        test_matrix = feat.bag_words2vec_bigram(full_vocab, reviews_test)
    else:
        test_matrix = feat.concatenate_feat(feat.bag_words2vec_unigram(
            full_vocab, reviews_test), feat.bag_words2vec_bigram(full_vocab, reviews_test))
    print('\ndescription of test matrix', stats.describe(test_matrix))

    train_labels = np.hstack((np.ones(train_size)*-1, np.ones(train_size)))
    X_train, y_train = train_matrix, train_labels

    print("\nrun the Grid Search for SVM classifier (with bow features) ... \n")
    tuned_parameters = [
        {'kernel': ['rbf'], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4], 'C': [10, 100, 1000, 10000]},
        {'kernel': ['linear'], 'C': [10, 100, 1000, 10000]}
        ]
    clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=5, n_jobs=12, verbose=10)
    clf.fit(X_train, y_train)
    print("\ngrid search, DONE.")
    print("\ngrid search results are listed as follows.")
    print(clf.best_params_, clf.best_score_)
    sorted(clf.cv_results_.keys())
    
    with smart_open(SVM_PARA_BOW_GRID_PATH, 'a+', encoding='utf-8') as f:
        line = str(feature_type) + "\t" + str(clf.best_params_) + "\t" + str(clf.best_score_)
        f.write(line)
        f.write("\n")
    print("\ngrid search results are written to file.")


def logistic_predictor(model_no):
    train_size, test_size, reviews_train, reviews_test = cv.prepare_data()
    train_matrix, train_labels = doc2vec.infer_embedding(model_no, reviews_train, train_size, False)
    test_matrix, test_labels = doc2vec.infer_embedding(model_no, reviews_test, test_size, False)
    # X_train, y_train, X_test, y_test = train_matrix, train_labels, test_matrix, test_labels

    logit = statsmodels.api.Logit(train_labels, train_matrix)
    predictor = logit.fit(disp=0)
    train_prediction = predictor.predict(train_matrix)
    cor = sum(np.rint(train_prediction) == train_labels)
    print("training accuracy", cor/len(train_prediction))
    test_predictions = predictor.predict(test_matrix)
    corrects = sum(np.rint(test_predictions) == test_labels)
    errors = len(test_predictions) - corrects
    error_rate = float(errors) / len(test_predictions)
    print("accuracy", corrects/len(test_predictions))


def save_results(feat_type, doc2vec, train_size, classification):
    if_doc2vec = 1 if doc2vec else 0
    notes = "results obtained on " + str(datetime.datetime.now())
    f = open('./results/results.txt', 'a+', encoding='utf-8')
    f.write("\nfeature: %s\tif_doc2vec: %d\ttraining size: %d\tclassification_accuracy: %f\tnotes: %s" % (
        feat_type, if_doc2vec, train_size, classification, notes))
    f.close()


def save_results_cv(fold_type, feat_type, doc2vec, results, performances, perf_average, variance):
    if_doc2vec = 1 if doc2vec else 0
    notes = "results obtained on " + str(datetime.datetime.now())
    f = open('./results/results_cv.txt', 'a+', encoding='utf-8')
    f.write("fold type: %s\nfeature: %s\t#performance: %s\taverage performance: %f\tvariance: %f\tnotes: %s\tdoc2vec: %d\n" % (fold_type, feat_type, performances, perf_average, variance, notes, if_doc2vec))
    f.write(str(results))
    f.write('\n')
    f.close()


'''
./svm-light/svm_learn [option] example_file model_file

-t (int)
0: linear (default)
1: polynomial (s a*b+c)^d
2. radial basis function exp(-gamma ||a-b||^2)
3. sigmoid tanh(s a*b + c)
4. user defined kernel from kernel.h

-g (float)
parameter gamma in rbf kernel

'''